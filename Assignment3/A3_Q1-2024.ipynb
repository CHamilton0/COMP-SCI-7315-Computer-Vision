{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMOJD0_jdzzg",
    "tags": []
   },
   "source": [
    "# Computer Vision 2024 Assignment 3: Deep Learning for Perception Tasks\n",
    "\n",
    "This assignment contains 2 questions. The first question probes understanding of deep learning for classification. The second question is a more challenging classification experiment on a larger dataset. Answer the questions in separate Python notebooks.\n",
    "\n",
    "## Question 1: A simple classifier, 20 marks\n",
    "\n",
    "For this exercise, we provide demo code showing how to train a network on a small dataset called [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist). Please run through the code \"tutorial-style\" to get a sense of what it is doing. Then use the code alongside lecture notes and other resources to understand how to use pytorch libraries to implement, train and use a neural network.\n",
    "\n",
    "For the Fashion-MNIST dataset the lables from 0-9 correspond to various clothing classes so you might find it convenient to create a python list as follows:\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "You will need to answer various questions about the system, how it operates, the results of experiments with it and make modifications to it yourself. You can change the training scheme and the network structure. \n",
    "\n",
    "Organize your own text and code cell to show the answer of each questions.\n",
    "\n",
    "Detailed requirements:\n",
    "\n",
    "**Q1.1 (1 point)**\n",
    "\n",
    "Extract 3 images of different types of clothing from the training dataset, print out the size/shape of the training images, and display the three with their corresponding labels. \n",
    "\n",
    "**Q1.2 (2 points)** \n",
    "\n",
    "Run the training code for 10 epochs, for different values of the learning rate. Fill in the table below and plot the loss curves for each experiment:\n",
    "\n",
    "|Lr|Accuracy|\n",
    "|---|---|\n",
    "|1   |      |\n",
    "|0.1|          |\n",
    "|0.01|         |\n",
    "|0.001  |        |\n",
    "\n",
    "\n",
    "**Q1.3 (3 points)** \n",
    "\n",
    "Report the number of epochs when the accuracy reaches 85%. Fill in the table below and plot the loass curve for each experiment:\n",
    "\n",
    "|Lr|Accuracy|Epoch|\n",
    "|---|---|---|\n",
    "|1   |      |     |\n",
    "|0.1|          |    |\n",
    "|0.01|         |    |\n",
    "|0.001  |        |     |\n",
    "\n",
    "\n",
    "**Q1.4 (2 points)** \n",
    "\n",
    "Compare the results in table 1 and table 2, what is your observation and your understanding of learning rate?\n",
    "\n",
    "\n",
    "**Q1.5 (5 points)** \n",
    "\n",
    "Build a wider network by modifying the code that constructs the network so that the hidden layer(s) contain more perceptrons, and record the accuracy along with the number of trainable parameters in your model.  Now modify the oroginal network to be deeper instead of wider (i.e. by adding more hidden layers). Record your accuracy and network size findings. Plot the loss curve for each experiment. Write down your conclusions about changing the network structure?  \n",
    "\n",
    "|Structures|Accuracy|Parameters|\n",
    "|---|---|---|\n",
    "|Base   |      ||\n",
    "|Deeper|          ||\n",
    "|Wider|         ||\n",
    "\n",
    "\n",
    "**Q1.6 (2 points)** \n",
    "\n",
    "Calculate the mean of the gradients of the loss to all trainable parameters. Plot the gradients curve for the first 100 training steps. What are your observations? Note that this gradients will be saved with the training weight automatically after you call loss.backwards(). Hint: the mean of the gradients decrease.\n",
    "\n",
    "For more exlanation of q1.7, you could refer to the following simple instructions: https://colab.research.google.com/drive/1XAsyNegGSvMf3_B6MrsXht7-fHqtJ7OW?usp=sharing\n",
    "\n",
    "**Q1.7 (5 points)** \n",
    "\n",
    "Modify the network structure and training/test to use a small convolutional neural network instead of an MLP. Discuss your findings with rehgard to convergence, accuracy and number of parameters, relative to MLPs.  \n",
    "\n",
    "Hint: Look at the structure of the CNN in the Workshop 3 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "-sqkIpLjpVsh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # This is for mathematical operations\n",
    "\n",
    "# this is used in plotting \n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "1wy3xhEx_x-1"
   },
   "outputs": [],
   "source": [
    "#### Tutorial Code\n",
    "####PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. \n",
    "#####Dataset stores samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download training data from open datasets. \n",
    "##Every TorchVision Dataset includes two arguments: \n",
    "##transform and target_transform to modify the samples and labels respectively.\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNI4IusI_1ol",
    "tags": []
   },
   "source": [
    "**NOTE**: For consistency with the original data set, we call our validation data \"test_data\". It is important to keep in mind though that we are using the data for model validation and not for testing the final, trained model (which requires data not used when training the model parameters). \n",
    "\n",
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset and supports automatic batching, sampling, shuffling, and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "nQZ5l5Zs_4C3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L1vl5rC52Un"
   },
   "source": [
    "Add in a code cell to inspect the training data, as per Q1.1. Each element of the training_data structure has a greyscale image (which you can use plt.imshow(img[0,:,:]) to display, just like you did in previous assignments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_defs = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "KpEhLSHg4Idw"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([28, 28])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([28, 28])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([28, 28])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code cell for training image display\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "import random\n",
    "training_indices = random.sample(range(batch_size), 3)\n",
    "\n",
    "training_images = [(images[index][0,:,:], labels[index]) for index in training_indices]\n",
    "\n",
    "for image in training_images:\n",
    "    plt.imshow(image[0], cmap=\"gray\")\n",
    "    plt.title(f\"Label: {label_defs[int(image[1])]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(f\"Shape: {image[0].shape}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMtCU2LO_9Dk"
   },
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the init function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "TRSp7pd3_6bS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "nYAnKhOfABZr"
   },
   "outputs": [],
   "source": [
    "###Define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFZYEHY7ADvS"
   },
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "L741B0uXAFrf"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        train_loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = train_loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "A44xKKnjAINf"
   },
   "outputs": [],
   "source": [
    "##Define a test function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "mJLACDm9AKxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304987  [    0/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[104], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     loss, current \u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem(), batch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:80\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     76\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 80\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:245\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 245\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:293\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m buf\n\u001b[0;32m--> 293\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train and test the model\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 epochs with learning rate: 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.315108  [    0/60000]\n",
      "loss:     nan  [ 6400/60000]\n",
      "loss:     nan  [12800/60000]\n",
      "loss:     nan  [19200/60000]\n",
      "loss:     nan  [25600/60000]\n",
      "loss:     nan  [32000/60000]\n",
      "loss:     nan  [38400/60000]\n",
      "loss:     nan  [44800/60000]\n",
      "loss:     nan  [51200/60000]\n",
      "loss:     nan  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss:      nan \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:     nan  [    0/60000]\n",
      "loss:     nan  [ 6400/60000]\n",
      "loss:     nan  [12800/60000]\n",
      "loss:     nan  [19200/60000]\n",
      "loss:     nan  [25600/60000]\n",
      "loss:     nan  [32000/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test(test_dataloader, model, loss_fn)\n\u001b[1;32m     19\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[124], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/CV/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train and test the model\n",
    "epochs = 2\n",
    "\n",
    "learning_rates = [1, 1e-1, 1e-2, 1e-3]\n",
    "\n",
    "for rate in learning_rates:\n",
    "    print(f\"Training for {epochs} epochs with learning rate: {rate}\")\n",
    "    model = NeuralNetwork().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    plt.plot(np.array(train_losses), 'r')\n",
    "    plt.plot(np.array(test_losses), 'b')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
